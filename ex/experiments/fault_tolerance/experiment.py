"""
å®¹é”™è¾¹ç•Œæµ‹è¯•å®éªŒ

å®éªŒç›®æ ‡ï¼š
1. æµ‹è¯•BFT4Agentåè®®çš„å®¹é”™è¾¹ç•Œï¼ˆç†è®ºä¸Šf/n â‰¤ 1/3ï¼‰
2. ä½¿ç”¨åˆé’»é—®é¢˜è¯±å¯¼è¯šå®agentä¹Ÿäº§ç”Ÿé”™è¯¯
3. åˆ†åˆ«æµ‹è¯•leaderä¸ºè¯šå®å’Œæ¶æ„ä¸¤ç§åœºæ™¯
4. ä½¿ç”¨çœŸå®LLMä½œä¸ºåç«¯

å…³é”®å‡è®¾ï¼š
- å³ä½¿è¯šå®çš„LLM agentä¹Ÿå¯èƒ½å› ä¸ºå¹»è§‰ã€ç†è§£åå·®è€ŒæŠ•é”™ç¥¨
- æµ‹è¯•ç³»ç»Ÿèƒ½å¦åœ¨"è¯šå®agentå‡ºé”™ + æ¶æ„agentæ”»å‡»"çš„åŒé‡å‹åŠ›ä¸‹è¾¾æˆå…±è¯†
"""

import sys
import os
import json
import time
import yaml
import re
from typing import Dict, List, Any, Tuple
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..'))
sys.path.insert(0, project_root)

from ex.utils import import_helper, Plotter
from ex.experiments.latency.consensus import BFT4AgentWithLatency


def expand_env_vars(config_value: Any) -> Any:
    """
    é€’å½’å±•å¼€é…ç½®ä¸­çš„ç¯å¢ƒå˜é‡
    æ”¯æŒ ${VAR_NAME} æ ¼å¼
    """
    if isinstance(config_value, str):
        def replace_env_var(match):
            var_name = match.group(1)
            return os.environ.get(var_name, match.group(0))

        return re.sub(r'\$\{([^}]+)\}', replace_env_var, config_value)
    elif isinstance(config_value, dict):
        return {k: expand_env_vars(v) for k, v in config_value.items()}
    elif isinstance(config_value, list):
        return [expand_env_vars(item) for item in config_value]
    else:
        return config_value


class FaultToleranceExperiment:
    """å®¹é”™è¾¹ç•Œæµ‹è¯•å®éªŒç±»"""

    def __init__(self, config_file: str = None, output_dir: str = "ex/results"):
        self.config_file = config_file
        self.output_dir = output_dir

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(f"{output_dir}/data", exist_ok=True)
        os.makedirs(f"{output_dir}/figures", exist_ok=True)

        # åŠ è½½é…ç½®
        self.config = self._load_config()

        # å®éªŒç»“æœ
        self.results = []

    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if self.config_file and os.path.exists(self.config_file):
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        else:
            # é»˜è®¤é…ç½®
            return {
                'experiment_name': 'fault_tolerance_boundary_test',
                'description': 'æµ‹è¯•BFT4Agentåè®®åœ¨é¢å¯¹åˆé’»é—®é¢˜æ—¶çš„å®¹é”™è¾¹ç•Œ',
                'variables': {
                    'num_agents': [9],  # å›ºå®š9ä¸ªèŠ‚ç‚¹ï¼ˆæ»¡è¶³3f+1ï¼‰
                    'malicious_count': [0, 1, 2, 3],  # 0%, 11%, 22%, 33%
                    'leader_type': ['honest', 'malicious'],  # æµ‹è¯•ä¸¤ç§leader
                    'network_delay': [[10, 100]],
                    'llm_backend': ['qwen']  # ä½¿ç”¨çœŸå®LLM
                },
                'tasks': {
                    'file': 'tricky_questions.json',  # ä¸“é—¨çš„åˆé’»é—®é¢˜æ•°æ®é›†
                    'num_tasks': 5,
                    'shuffle': True
                },
                'global': {
                    'timeout': 60.0,  # çœŸå®LLMéœ€è¦æ›´é•¿è¶…æ—¶
                    'max_retries': 10,  # å…è®¸æ›´å¤šé‡è¯•
                }
            }

    def run(self):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        print(f"\n{'='*80}")
        print(f"å®¹é”™è¾¹ç•Œæµ‹è¯•å®éªŒ")
        print(f"å®éªŒåç§°: {self.config['experiment_name']}")
        print(f"å®éªŒæè¿°: {self.config.get('description', '')}")
        print(f"{'='*80}\n")

        # è·å–å˜é‡ç»„åˆ
        variables = self.config['variables']
        num_agents_list = variables.get('num_agents', [9])
        malicious_count_list = variables.get('malicious_count', [0, 1, 2, 3])
        leader_type_list = variables.get('leader_type', ['honest', 'malicious'])
        network_delay_list = variables.get('network_delay', [[10, 100]])
        llm_backend_list = variables.get('llm_backend', ['qwen'])

        from itertools import product
        combinations = list(product(
            num_agents_list, malicious_count_list, leader_type_list,
            network_delay_list, llm_backend_list
        ))

        print(f"æ€»å…± {len(combinations)} ä¸ªå®éªŒé…ç½®\n")

        # è¿è¡Œæ¯ä¸ªé…ç½®
        for i, (num_agents, mal_count, leader_type, net_delay, llm_backend) in enumerate(combinations, 1):
            malicious_ratio = mal_count / num_agents
            print(f"\n{'='*80}")
            print(f"é…ç½® {i}/{len(combinations)}")
            print(f"  èŠ‚ç‚¹æ•°: {num_agents}")
            print(f"  æ¶æ„èŠ‚ç‚¹æ•°: {mal_count} ({malicious_ratio:.1%})")
            print(f"  Leaderç±»å‹: {leader_type}")
            print(f"  ç½‘ç»œå»¶è¿Ÿ: {net_delay}ms")
            print(f"  LLMåç«¯: {llm_backend}")
            print(f"{'='*80}")

            result = self._run_single_config(
                num_agents, malicious_ratio, net_delay, llm_backend,
                mal_count, leader_type
            )
            self.results.append(result)

        # ä¿å­˜ç»“æœå¹¶åˆ†æ
        output_file = self._save_results()
        self._analyze_results(output_file)

    def _run_single_config(
        self, num_agents: int, malicious_ratio: float,
        network_delay: tuple, llm_backend: str,
        malicious_count: int, leader_type: str
    ) -> Dict:
        """è¿è¡Œå•ä¸ªé…ç½®"""
        # åˆ‡æ¢åˆ°bft4agent-simpleç›®å½•
        original_dir = os.getcwd()
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..'))
        bft4agent_dir = os.path.join(project_root, 'bft4agent-simple')
        os.chdir(bft4agent_dir)

        try:
            # åˆ›å»ºLLM
            if llm_backend == "mock":
                llm = import_helper.LLMCaller(
                    backend="mock",
                    accuracy=self.config['global'].get('mock_accuracy', 0.8)
                )
            else:
                llm_api_config = self.config.get('llm_api_config', {}).get(llm_backend, {})
                llm_api_config = expand_env_vars(llm_api_config)
                llm = import_helper.LLMCaller(backend=llm_backend, **llm_api_config)

            # åˆ›å»ºAgent
            agents = import_helper.create_agents(
                num_agents=num_agents,
                malicious_ratio=malicious_ratio,
                llm_caller=llm,
                role_configs=[],
                random_assignment=True
            )

            # å¦‚æœéœ€è¦ç¡®ä¿leaderæ˜¯è¯šå®/æ¶æ„çš„ï¼Œå¼ºåˆ¶è®¾ç½®ç¬¬ä¸€ä¸ªagent
            if leader_type == 'malicious' and not agents[0].is_malicious:
                # å°†ç¬¬ä¸€ä¸ªagentè®¾ä¸ºæ¶æ„ï¼Œå¹¶è°ƒæ•´å…¶ä»–agent
                agents[0].is_malicious = True
                # ä»è¯šå®agentä¸­éšæœºé€‰ä¸€ä¸ªæ”¹ä¸ºæ¶æ„ä»¥ä¿æŒæ€»æ•°
                for i in range(1, len(agents)):
                    if not agents[i].is_malicious:
                        agents[i].is_malicious = False
                        break
            elif leader_type == 'honest' and agents[0].is_malicious:
                # å°†ç¬¬ä¸€ä¸ªagentè®¾ä¸ºè¯šå®ï¼Œå¹¶è°ƒæ•´å…¶ä»–agent
                agents[0].is_malicious = False
                # ä»æ¶æ„agentä¸­éšæœºé€‰ä¸€ä¸ªæ”¹ä¸ºè¯šå®ä»¥ä¿æŒæ€»æ•°
                for i in range(1, len(agents)):
                    if agents[i].is_malicious:
                        agents[i].is_malicious = True
                        break

            # åˆ›å»ºç½‘ç»œ
            network = import_helper.Network(delay_range=network_delay, packet_loss=0.01)
            for agent in agents:
                network.register(agent)

            # åˆ›å»ºBFT
            bft = BFT4AgentWithLatency(
                agents=agents,
                network=network,
                timeout=self.config['global']['timeout'],
                max_retries=self.config['global']['max_retries']
            )

            # åŠ è½½ä»»åŠ¡
            num_tasks = self.config['tasks']['num_tasks']
            all_tasks = import_helper.TaskLoader({'tasks': self.config['tasks']}).load()

            # å¦‚æœæ˜¯å¿«é€Ÿæµ‹è¯•ï¼Œé™åˆ¶ä»»åŠ¡æ•°
            if self.config.get('quick_test', False):
                num_tasks = min(3, len(all_tasks))

            # è¿è¡Œä»»åŠ¡
            task_results = []
            for i, task in enumerate(all_tasks[:num_tasks], 1):
                print(f"\n  ä»»åŠ¡ {i}/{num_tasks}: {task['content'][:60]}...")
                print(f"  ç±»å‹: {task.get('type', 'unknown')}, éš¾åº¦: {task.get('difficulty', 'unknown')}")

                # è®°å½•leaderä¿¡æ¯
                leader_idx = bft.current_view % len(agents)
                leader_is_malicious = agents[leader_idx].is_malicious

                result = bft.run(task)
                result['leader_is_malicious'] = leader_is_malicious
                result['task_type'] = task.get('type', 'unknown')
                result['task_difficulty'] = task.get('difficulty', 'unknown')
                result['expected_behavior'] = task.get('expected_behavior', 'correct')

                task_results.append(result)

                # æ‰“å°å…³é”®ç»Ÿè®¡
                if result['success']:
                    prep_data = result['latency_data'].get('prepare', {})
                    y_count = prep_data.get('y_count', 0)
                    n_count = prep_data.get('n_count', 0)
                    print(f"    ç»“æœ: æˆåŠŸ | Yç¥¨: {y_count}, Nç¥¨: {n_count}")
                else:
                    print(f"    ç»“æœ: å¤±è´¥ | è§†å›¾åˆ‡æ¢: {result.get('view_changes', 0)}æ¬¡")

                time.sleep(0.5)

            # ç»Ÿè®¡
            success_count = sum(1 for r in task_results if r['success'])

            # è®¡ç®—å„ç§ç»Ÿè®¡æŒ‡æ ‡
            successful_results = [r for r in task_results if r['success']]
            all_results_for_voting = task_results  # åŒ…æ‹¬å¤±è´¥çš„ä»»åŠ¡ä¹Ÿåˆ†ææŠ•ç¥¨

            # æŠ•ç¥¨ç»Ÿè®¡ï¼ˆåŒ…æ‹¬æˆåŠŸå’Œå¤±è´¥çš„ä»»åŠ¡ï¼‰
            vote_details = []
            y_counts = []
            n_counts = []
            margins = []  # è·ç¦»2f+1é˜ˆå€¼çš„ä½™é‡

            # è®¡ç®—é˜ˆå€¼
            f = malicious_count
            quorum = 2 * f + 1

            for r in all_results_for_voting:
                # å®‰å…¨è·å–latency_data
                latency_data = r.get('latency_data', {})
                if not latency_data:
                    # å¦‚æœæ²¡æœ‰latency_dataï¼ˆä»»åŠ¡å¤±è´¥ï¼‰ï¼Œè·³è¿‡æˆ–ä½¿ç”¨é»˜è®¤å€¼
                    if not r.get('success'):
                        # å¤±è´¥çš„ä»»åŠ¡ï¼Œè®°å½•é»˜è®¤å€¼
                        y_count = 0
                        n_count = num_agents  # å‡è®¾æ‰€æœ‰äººéƒ½æŠ•äº†Nç¥¨
                    else:
                        # æˆåŠŸä½†æ— æ•°æ®ï¼Œè·³è¿‡
                        continue
                else:
                    prep_data = latency_data.get('prepare', {})
                    y_count = prep_data.get('y_count', 0)
                    n_count = prep_data.get('n_count', 0)

                total_votes = y_count + n_count

                # è®¡ç®—ä½™é‡ï¼ˆè·ç¦»é˜ˆå€¼è¿˜å·®å¤šå°‘ç¥¨ï¼‰
                margin = y_count - quorum if y_count >= quorum else quorum - y_count

                vote_details.append({
                    'task_success': r.get('success', False),
                    'y_count': y_count,
                    'n_count': n_count,
                    'total_votes': total_votes,
                    'quorum': quorum,
                    'margin': margin,
                    'is_consensus_reached': y_count >= quorum
                })

                y_counts.append(y_count)
                n_counts.append(n_count)
                margins.append(margin)

            # è®¡ç®—å¹³å‡å€¼
            if successful_results:
                avg_total_latency = sum(r['latency_data']['total'] for r in successful_results) / len(successful_results)
                avg_prepare_latency = sum(r['latency_data']['prepare']['latency'] for r in successful_results) / len(successful_results)
                avg_commit_latency = sum(r['latency_data']['commit']['latency'] for r in successful_results) / len(successful_results)
                avg_view_changes = sum(r['view_changes'] for r in successful_results) / len(successful_results)
            else:
                avg_total_latency = 0
                avg_prepare_latency = 0
                avg_commit_latency = 0
                avg_view_changes = 0

            # æŠ•ç¥¨ç»Ÿè®¡
            avg_y_count = sum(y_counts) / len(y_counts) if y_counts else 0
            avg_n_count = sum(n_counts) / len(n_counts) if n_counts else 0
            avg_margin = sum(margins) / len(margins) if margins else 0
            min_margin = min(margins) if margins else 0

            print(f"\n  ç»“æœæ±‡æ€»:")
            print(f"    æˆåŠŸ: {success_count}/{len(task_results)} ({success_count/len(task_results)*100:.1f}%)")
            print(f"    å¹³å‡æ€»å»¶è¿Ÿ: {avg_total_latency:.3f}ç§’")
            print(f"    å¹³å‡è§†å›¾åˆ‡æ¢: {avg_view_changes:.1f}æ¬¡")
            print(f"  æŠ•ç¥¨ç»Ÿè®¡:")
            print(f"    æ³•å®šäººæ•°é˜ˆå€¼(2f+1): {quorum}")
            print(f"    å¹³å‡Yç¥¨: {avg_y_count:.1f}, å¹³å‡Nç¥¨: {avg_n_count:.1f}")
            print(f"    å¹³å‡ä½™é‡: +{avg_margin:.1f}ç¥¨ (è·ç¦»é˜ˆå€¼)")
            print(f"    æœ€å°ä½™é‡: +{min_margin:.1f}ç¥¨ (æœ€æ¥è¿‘é˜ˆå€¼çš„ä¸€æ¬¡)")

            return {
                'config': {
                    'num_agents': num_agents,
                    'malicious_count': malicious_count,
                    'malicious_ratio': malicious_ratio,
                    'leader_type': leader_type,
                    'network_delay': network_delay,
                    'llm_backend': llm_backend
                },
                'task_results': task_results,
                'vote_details': vote_details,
                'summary': {
                    'total_tasks': len(task_results),
                    'success_count': success_count,
                    'success_rate': success_count / len(task_results) if len(task_results) > 0 else 0,
                    'avg_total_latency': avg_total_latency,
                    'avg_prepare_latency': avg_prepare_latency,
                    'avg_commit_latency': avg_commit_latency,
                    'avg_view_changes': avg_view_changes,
                    # æŠ•ç¥¨ç»Ÿè®¡
                    'quorum_threshold': quorum,
                    'avg_y_count': avg_y_count,
                    'avg_n_count': avg_n_count,
                    'avg_margin': avg_margin,
                    'min_margin': min_margin,
                }
            }

        finally:
            # æ¢å¤å·¥ä½œç›®å½•
            os.chdir(original_dir)

    def _generate_filename(self, timestamp: str) -> str:
        """ç”ŸæˆåŒ…å«å®éªŒä¿¡æ¯çš„æ–‡ä»¶å"""
        exp_name = self.config.get('experiment_name', 'fault_tolerance_test')
        return f"{exp_name}_{timestamp}"

    def _save_results(self) -> str:
        """ä¿å­˜ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = self._generate_filename(timestamp)
        output_file = f"{self.output_dir}/data/{filename}.json"

        data = {
            'experiment_name': self.config['experiment_name'],
            'description': self.config.get('description', ''),
            'timestamp': datetime.now().isoformat(),
            'config': self.config,
            'results': self.results
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        print(f"\n{'='*80}")
        print(f"ç»“æœå·²ä¿å­˜: {output_file}")
        print(f"{'='*80}")

        # ä¹Ÿä¿å­˜ä¸ºlatest
        latest_file = f"{self.output_dir}/data/fault_tolerance_latest.json"
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        return output_file

    def run_quick_test(self):
        """è¿è¡Œå¿«é€Ÿæµ‹è¯•ï¼ˆä½¿ç”¨Mock LLMï¼‰"""
        print("\nå¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼ˆä½¿ç”¨Mock LLMï¼‰")
        print("="*60)

        # ç®€åŒ–é…ç½®
        self.config = {
            'experiment_name': 'fault_tolerance_quick_test',
            'description': 'å¿«é€Ÿæµ‹è¯•å®¹é”™è¾¹ç•Œ',
            'variables': {
                'num_agents': [7],
                'malicious_count': [0, 1, 2],
                'leader_type': ['honest'],  # åªæµ‹è¯•è¯šå®leader
                'network_delay': [[10, 100]],
                'llm_backend': ['mock']
            },
            'tasks': {
                'file': 'tricky_questions.json',
                'num_tasks': 3,
                'shuffle': False
            },
            'global': {
                'timeout': 30.0,
                'max_retries': 5,
                'mock_accuracy': 0.7  # Mock LLMå‡†ç¡®ç‡è¾ƒä½ï¼Œæ¨¡æ‹Ÿåˆé’»é—®é¢˜
            }
        }
        self.config['quick_test'] = True

        self.run()

    def _analyze_results(self, result_file: str):
        """åˆ†æç»“æœå¹¶ç”Ÿæˆå¯è§†åŒ–"""
        print(f"\n{'='*80}")
        print(f"åˆ†æå®éªŒç»“æœ")
        print(f"{'='*80}\n")

        # åŠ è½½ç»“æœ
        with open(result_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # æå–å®éªŒå…ƒæ•°æ®
        # experiment_name = data.get('experiment_name', 'fault_tolerance_test')
        # timestamp = data.get('timestamp', datetime.now().isoformat())
        # llm_backend = data.get('config', {}).get('variables', {}).get('llm_backend', ['unknown'])[0]
        # num_agents = data.get('config', {}).get('variables', {}).get('num_agents', [9])[0]

        # æå–å¹¶æ•´ç†æ•°æ®
        records = []
        for exp_result in data['results']:
            config = exp_result['config']
            summary = exp_result['summary']

            # å®‰å…¨åœ°æå–å€¼ï¼Œç¡®ä¿ç±»å‹æ­£ç¡®
            def safe_get_float(d, key, default=0.0):
                val = d.get(key, default)
                if isinstance(val, dict):
                    print(f"Warning: {key} is dict, using default")
                    return default
                try:
                    return float(val or 0)
                except (TypeError, ValueError):
                    return default

            record = {
                'malicious_count': config['malicious_count'],
                'malicious_ratio': config['malicious_ratio'],  # ä»configä¸­æå–
                'leader_type': config['leader_type'],
                'num_agents': config['num_agents'],
                'success_rate': safe_get_float(summary, 'success_rate', 0.0),
                'avg_total_latency': safe_get_float(summary, 'avg_total_latency', 0.0),
                'avg_view_changes': safe_get_float(summary, 'avg_view_changes', 0.0),
                # æŠ•ç¥¨ç»Ÿè®¡
                'quorum_threshold': safe_get_float(summary, 'quorum_threshold', 0),
                'avg_y_count': safe_get_float(summary, 'avg_y_count', 0),
                'avg_n_count': safe_get_float(summary, 'avg_n_count', 0),
                'avg_margin': safe_get_float(summary, 'avg_margin', 0),
                'min_margin': safe_get_float(summary, 'min_margin', 0),
                'vote_details': exp_result.get('vote_details', []),
            }
            records.append(record)

        # åˆ†åˆ«åˆ†æhonest leaderå’Œmalicious leader
        honest_leader_results = [r for r in records if r['leader_type'] == 'honest']
        malicious_leader_results = [r for r in records if r['leader_type'] == 'malicious']

        # æ‰“å°ç»Ÿè®¡è¡¨æ ¼
        print(f"{'='*100}")
        print(f"å®éªŒç»“æœæ±‡æ€»è¡¨")
        print(f"{'='*100}")

        print(f"\nã€è¯šå®Leaderåœºæ™¯ã€‘")
        print(f"{'æ¶æ„':<6} {'æ¶æ„æ¯”ä¾‹':<10} {'æˆåŠŸç‡':<10} {'Y/Nå¹³å‡':<10} {'å¹³å‡ä½™é‡':<12} {'æœ€å°ä½™é‡':<10}")
        print(f"{'èŠ‚ç‚¹æ•°':<6} {'':<10} {'':<10} {'æŠ•ç¥¨':<10} {'(2f+1)':<12} {'(æœ€å±é™©)':<10}")
        print(f"{'-'*70}")
        for r in sorted(honest_leader_results, key=lambda x: x['malicious_count']):
            # å®‰å…¨è·å–å€¼ï¼Œé¿å…Noneæˆ–dictç±»å‹é”™è¯¯
            avg_y = r.get('avg_y_count', 0) or 0
            avg_n = r.get('avg_n_count', 0) or 0
            avg_marg = r.get('avg_margin', 0) or 0
            min_marg = r.get('min_margin', 0) or 0

            # ç¡®ä¿æ˜¯æ•°å­—ç±»å‹
            avg_y = float(avg_y) if not isinstance(avg_y, dict) else 0
            avg_n = float(avg_n) if not isinstance(avg_n, dict) else 0
            avg_marg = float(avg_marg) if not isinstance(avg_marg, dict) else 0
            min_marg = float(min_marg) if not isinstance(min_marg, dict) else 0

            print(f"{r['malicious_count']:<6} {r['malicious_ratio']:>8.1%} "
                  f"{r['success_rate']:>8.1%} {avg_y:>4.0f}/{avg_n:<4.0f} "
                  f"+{avg_marg:>5.1f}ç¥¨    +{min_marg:>5.1f}ç¥¨")

        if malicious_leader_results:
            print(f"\nã€æ¶æ„Leaderåœºæ™¯ã€‘")
            print(f"{'æ¶æ„':<6} {'æ¶æ„æ¯”ä¾‹':<10} {'æˆåŠŸç‡':<10} {'Y/Nå¹³å‡':<10} {'å¹³å‡ä½™é‡':<12} {'æœ€å°ä½™é‡':<10}")
            print(f"{'èŠ‚ç‚¹æ•°':<6} {'':<10} {'':<10} {'æŠ•ç¥¨':<10} {'(2f+1)':<12} {'(æœ€å±é™©)':<10}")
            print(f"{'-'*70}")
            for r in sorted(malicious_leader_results, key=lambda x: x['malicious_count']):
                # å®‰å…¨è·å–å€¼
                avg_y = r.get('avg_y_count', 0) or 0
                avg_n = r.get('avg_n_count', 0) or 0
                avg_marg = r.get('avg_margin', 0) or 0
                min_marg = r.get('min_margin', 0) or 0

                # ç¡®ä¿æ˜¯æ•°å­—ç±»å‹
                avg_y = float(avg_y) if not isinstance(avg_y, dict) else 0
                avg_n = float(avg_n) if not isinstance(avg_n, dict) else 0
                avg_marg = float(avg_marg) if not isinstance(avg_marg, dict) else 0
                min_marg = float(min_marg) if not isinstance(min_marg, dict) else 0

                print(f"{r['malicious_count']:<6} {r['malicious_ratio']:>8.1%} "
                      f"{r['success_rate']:>8.1%} {avg_y:>4.0f}/{avg_n:<4.0f} "
                      f"+{avg_marg:>5.1f}ç¥¨    +{min_marg:>5.1f}ç¥¨")

        print(f"\n{'='*100}\n")

        # ç»˜å›¾ï¼ˆæš‚æ—¶ç¦ç”¨ï¼Œmatplotlibå…¼å®¹æ€§é—®é¢˜ï¼‰
        print(f"ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        print(f"æ³¨æ„ï¼šç”±äºmatplotlibå…¼å®¹æ€§é—®é¢˜ï¼Œå¤æ‚å¯è§†åŒ–æš‚æ—¶ç¦ç”¨")
        print(f"æ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ°JSONæ–‡ä»¶ï¼Œå¯ä½¿ç”¨å…¶ä»–å·¥å…·ï¼ˆå¦‚Excelï¼‰è¿›è¡Œå¯è§†åŒ–")
        # plotter = Plotter(output_dir=f"{self.output_dir}/figures")
        # self._plot_voting_analysis(...)
        print(f"\næ•°æ®å·²ä¿å­˜åˆ°: {self.output_dir}/data/")
        print(f"å»ºè®®ä½¿ç”¨Python pandas/matplotlib æˆ– Excel è¿›è¡Œåç»­å¯è§†åŒ–åˆ†æ")

    def _plot_voting_analysis(
        self,
        honest_results: List[Dict],
        malicious_results: List[Dict],
        plotter: Plotter,
        timestamp: str = None,
        experiment_name: str = "fault_tolerance_test",
        llm_backend: str = "unknown",
        num_agents: int = 9
    ):
        """ç»˜åˆ¶æŠ•ç¥¨åˆ†æå’Œå®¹é”™è¾¹ç•Œå¯è§†åŒ–"""
        import matplotlib.pyplot as plt
        import matplotlib
        matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        matplotlib.rcParams['axes.unicode_minus'] = False

        # æŒ‰æ¶æ„æ¯”ä¾‹æ’åº
        honest_results.sort(key=lambda x: x['malicious_ratio'])
        malicious_results.sort(key=lambda x: x['malicious_ratio'])

        # ç”Ÿæˆå¸¦æ—¶é—´æˆ³å’Œé…ç½®çš„æ–‡ä»¶å
        if timestamp:
            dt = datetime.fromisoformat(timestamp)
            time_str = dt.strftime('%Y%m%d_%H%M%S')
        else:
            time_str = datetime.now().strftime('%Y%m%d_%H%M%S')

        filename = f"fault_tolerance_voting_analysis_{experiment_name}_{num_agents}agents_{llm_backend}_{time_str}.png"

        # è°ƒè¯•ï¼šæ‰“å°å…³é”®æ•°æ®ç±»å‹
        print(f"\n[è°ƒè¯•] ç»˜å›¾å‚æ•°:")
        print(f"  honest_resultsæ•°é‡: {len(honest_results)}")
        print(f"  malicious_resultsæ•°é‡: {len(malicious_results)}")
        print(f"  plotter.output_dirç±»å‹: {type(plotter.output_dir)}")

        # åˆ›å»º2x3å­å›¾
        fig = plt.figure(figsize=(18, 12))
        gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)
        fig.suptitle('BFT4Agent å®¹é”™è¾¹ç•Œä¸æŠ•ç¥¨åˆ†æï¼ˆåˆé’»é—®é¢˜åœºæ™¯ï¼‰', fontsize=16, fontweight='bold')

        # ===== å›¾1: æˆåŠŸç‡ vs æ¶æ„æ¯”ä¾‹ =====
        ax1 = fig.add_subplot(gs[0, 0])
        x_data = [float(r['malicious_ratio']) for r in honest_results]
        y_data = [float(r['success_rate']) for r in honest_results]
        ax1.plot(x_data, y_data, marker='o', linewidth=2.5, markersize=8,
                color='#2E86AB', label='è¯šå®Leader')
        if malicious_results:
            x_m = [float(r['malicious_ratio']) for r in malicious_results]
            y_m = [float(r['success_rate']) for r in malicious_results]
            ax1.plot(x_m, y_m, marker='s', linewidth=2.5, markersize=8,
                    color='#A23B72', label='æ¶æ„Leader')
        ax1.axvline(x=1/3, color='r', linestyle='--', alpha=0.5, label='ç†è®ºé˜ˆå€¼(33%)')
        ax1.axhline(y=2/3, color='g', linestyle=':', alpha=0.3, label='æœ€ä½æˆåŠŸé˜ˆå€¼(66.7%)')
        ax1.set_xlabel('æ¶æ„èŠ‚ç‚¹æ¯”ä¾‹', fontsize=11)
        ax1.set_ylabel('å…±è¯†æˆåŠŸç‡', fontsize=11)
        ax1.set_title('å®¹é”™è¾¹ç•Œï¼šæˆåŠŸç‡ vs æ¶æ„æ¯”ä¾‹', fontsize=12, fontweight='bold')
        ax1.set_ylim([0, 1.05])
        ax1.grid(True, alpha=0.3)
        ax1.legend(fontsize=9)

        # ===== å›¾1: æˆåŠŸç‡ vs æ¶æ„æ¯”ä¾‹ =====
        ax1 = fig.add_subplot(gs[0, 0])
        ax1.plot([r['malicious_ratio'] for r in honest_results],
                [r['success_rate'] for r in honest_results],
                marker='o', linewidth=2.5, markersize=8, color='#2E86AB',
                label='è¯šå®Leader')
        if malicious_results:  # åªæœ‰åœ¨æœ‰æ¶æ„Leaderæ•°æ®æ—¶æ‰ç»˜åˆ¶
            ax1.plot([r['malicious_ratio'] for r in malicious_results],
                    [r['success_rate'] for r in malicious_results],
                    marker='s', linewidth=2.5, markersize=8, color='#A23B72',
                    label='æ¶æ„Leader')
        ax1.axvline(x=1/3, color='r', linestyle='--', alpha=0.5, label='ç†è®ºé˜ˆå€¼(33%)')
        ax1.axhline(y=2/3, color='g', linestyle=':', alpha=0.3, label='æœ€ä½æˆåŠŸé˜ˆå€¼(66.7%)')
        ax1.set_xlabel('æ¶æ„èŠ‚ç‚¹æ¯”ä¾‹', fontsize=11)
        ax1.set_ylabel('å…±è¯†æˆåŠŸç‡', fontsize=11)
        ax1.set_title('å®¹é”™è¾¹ç•Œï¼šæˆåŠŸç‡ vs æ¶æ„æ¯”ä¾‹', fontsize=12, fontweight='bold')
        ax1.set_ylim([0, 1.05])
        ax1.grid(True, alpha=0.3)
        ax1.legend(fontsize=9)

        # ===== å›¾2: æŠ•ç¥¨åˆ†å¸ƒï¼ˆYç¥¨ vs Nç¥¨ï¼‰ =====
        ax2 = fig.add_subplot(gs[0, 1])
        malicious_ratios = [r['malicious_ratio'] for r in honest_results]
        y_counts_honest = [float(r.get('avg_y_count', 0) or 0) for r in honest_results]
        n_counts_honest = [float(r.get('avg_n_count', 0) or 0) for r in honest_results]

        x = range(len(malicious_ratios))
        width = 0.35

        ax2.bar([i - width/2 for i in x], y_counts_honest, width,
               label='Yç¥¨ï¼ˆåŒæ„ï¼‰', color='#2E86AB', alpha=0.8)
        ax2.bar([i + width/2 for i in x], n_counts_honest, width,
               label='Nç¥¨ï¼ˆåå¯¹ï¼‰', color='#C73E1D', alpha=0.8)

        # æ·»åŠ 2f+1é˜ˆå€¼çº¿
        for i, r in enumerate(honest_results):
            quorum = r['quorum_threshold']
            ax2.axhline(y=quorum, xmin=(i-0.5)/len(malicious_ratios),
                       xmax=(i+0.5)/len(malicious_ratios),
                       color='r', linestyle='--', alpha=0.3, linewidth=2)
            # æ ‡æ³¨é˜ˆå€¼
            if i == 0:
                ax2.text(i, quorum + 0.2, f'2f+1={quorum}',
                        ha='center', fontsize=8, color='r')

        ax2.set_xlabel('æ¶æ„èŠ‚ç‚¹æ¯”ä¾‹', fontsize=11)
        ax2.set_ylabel('å¹³å‡ç¥¨æ•°', fontsize=11)
        ax2.set_title('æŠ•ç¥¨åˆ†å¸ƒï¼šYç¥¨ vs Nç¥¨ï¼ˆè¯šå®Leaderï¼‰', fontsize=12, fontweight='bold')
        ax2.set_xticks(x)
        ax2.set_xticklabels([f'{int(r*100)}%' for r in malicious_ratios])
        ax2.legend(fontsize=9)
        ax2.grid(True, alpha=0.3, axis='y')

        # ===== å›¾3: ä½™é‡åˆ†æï¼ˆè·ç¦»2f+1çš„ä½™é‡ï¼‰ =====
        ax3 = fig.add_subplot(gs[0, 2])
        margins_avg = [float(r.get('avg_margin', 0) or 0) for r in honest_results]
        margins_min = [float(r.get('min_margin', 0) or 0) for r in honest_results]

        ax3.plot(malicious_ratios, margins_avg,
                marker='o', linewidth=2.5, markersize=8, color='#2E86AB',
                label='å¹³å‡ä½™é‡')
        ax3.plot(malicious_ratios, margins_min,
                marker='v', linewidth=2, markersize=6, color='#F18F01',
                label='æœ€å°ä½™é‡ï¼ˆæœ€å±é™©ï¼‰')
        ax3.axhline(y=0, color='r', linestyle='--', alpha=0.5, label='é›¶ä½™é‡ï¼ˆå±é™©çº¿ï¼‰')

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (ratio, avg, min_m) in enumerate(zip(malicious_ratios, margins_avg, margins_min)):
            ax3.annotate(f'+{avg:.1f}', (ratio, avg),
                        textcoords="offset points", xytext=(0, 5),
                        ha='center', fontsize=8, color='#2E86AB')
            ax3.annotate(f'+{min_m:.1f}', (ratio, min_m),
                        textcoords="offset points", xytext=(0, -15),
                        ha='center', fontsize=8, color='#F18F01')

        ax3.set_xlabel('æ¶æ„èŠ‚ç‚¹æ¯”ä¾‹', fontsize=11)
        ax3.set_ylabel('ä½™é‡ï¼ˆç¥¨æ•°ï¼‰', fontsize=11)
        ax3.set_title('å®¹é”™ä½™é‡ï¼šè·ç¦»2f+1é˜ˆå€¼è¿˜å·®å¤šå°‘ç¥¨', fontsize=12, fontweight='bold')
        ax3.grid(True, alpha=0.3)
        ax3.legend(fontsize=9)

        # ===== å›¾4: è¯šå® vs æ¶æ„LeaderæŠ•ç¥¨å¯¹æ¯” =====
        ax4 = fig.add_subplot(gs[1, 0])

        # ç¡®ä¿æ•°æ®æ˜¯æ•°å­—ç±»å‹
        y_counts_honest = [float(r.get('avg_y_count', 0) or 0) for r in honest_results]

        ax4.plot(malicious_ratios, y_counts_honest,
                marker='o', linewidth=2.5, markersize=7, color='#2E86AB',
                label='è¯šå®Leader-Yç¥¨')

        if malicious_results:  # åªæœ‰åœ¨æœ‰æ¶æ„Leaderæ•°æ®æ—¶æ‰ç»˜åˆ¶
            y_counts_malicious = [float(r.get('avg_y_count', 0) or 0) for r in malicious_results]
            ax4.plot(malicious_ratios, y_counts_malicious,
                    marker='s', linewidth=2.5, markersize=7, color='#A23B72',
                    label='æ¶æ„Leader-Yç¥¨')

        # æ·»åŠ é˜ˆå€¼çº¿
        for i, r in enumerate(honest_results):
            quorum = r['quorum_threshold']
            ax4.axhline(y=quorum, xmin=(i-0.5)/len(malicious_ratios),
                       xmax=(i+0.5)/len(malicious_ratios),
                       color='r', linestyle='--', alpha=0.3, linewidth=1.5)

        ax4.set_xlabel('æ¶æ„èŠ‚ç‚¹æ¯”ä¾‹', fontsize=11)
        ax4.set_ylabel('å¹³å‡Yç¥¨æ•°', fontsize=11)
        if malicious_results:
            ax4.set_title('Leaderç±»å‹å¯¹æ¯”ï¼šYç¥¨è·å–èƒ½åŠ›', fontsize=12, fontweight='bold')
        else:
            ax4.set_title('Leaderç±»å‹å¯¹æ¯”ï¼šYç¥¨è·å–èƒ½åŠ›ï¼ˆä»…è¯šå®Leaderï¼‰', fontsize=12, fontweight='bold')
        ax4.set_xticks(malicious_ratios)
        ax4.set_xticklabels([f'{int(r*100)}%' for r in malicious_ratios])
        ax4.grid(True, alpha=0.3)
        ax4.legend(fontsize=9)

        # ===== å›¾5: ä½™é‡æŸ±çŠ¶å›¾ï¼ˆæ¸…æ™°å±•ç¤ºå®‰å…¨è¾¹ç•Œï¼‰ =====
        ax5 = fig.add_subplot(gs[1, 1])

        categories = [f'{int(r*100)}%\n({r["malicious_count"]}ä¸ª)' for r in honest_results]
        x_pos = range(len(categories))

        # ç¡®ä¿æ•°æ®æ˜¯æ•°å­—ç±»å‹
        margins_avg_safe = [float(r.get('avg_margin', 0) or 0) for r in honest_results]

        bars = ax5.bar(x_pos, margins_avg_safe, color='#2E86AB', alpha=0.7,
                      edgecolor='black', linewidth=1.5, label='å¹³å‡ä½™é‡')

        # æ·»åŠ é˜ˆå€¼çº¿
        ax5.axhline(y=0, color='r', linestyle='--', linewidth=2, label='å±é™©çº¿ï¼ˆé›¶ä½™é‡ï¼‰')

        # æ·»åŠ æ•°å€¼æ ‡ç­¾å’Œé¢œè‰²æŒ‡ç¤º
        for i, (bar, margin) in enumerate(zip(bars, margins_avg_safe)):
            height = bar.get_height()
            # æ ¹æ®ä½™é‡å¤§å°è®¾ç½®é¢œè‰²ï¼ˆä½™é‡è¶Šå°è¶Šå±é™©ï¼‰
            if margin < 1:
                color = '#C73E1D'  # çº¢è‰² - å±é™©
            elif margin < 2:
                color = '#F18F01'  # æ©™è‰² - è­¦å‘Š
            else:
                color = '#2E86AB'  # è“è‰² - å®‰å…¨
            bar.set_color(color)

            ax5.text(bar.get_x() + bar.get_width()/2., height,
                    f'+{margin:.1f}',
                    ha='center', va='bottom', fontsize=10, fontweight='bold')

        ax5.set_xlabel('æ¶æ„èŠ‚ç‚¹æ¯”ä¾‹', fontsize=11)
        ax5.set_ylabel('å¹³å‡ä½™é‡ï¼ˆç¥¨æ•°ï¼‰', fontsize=11)
        ax5.set_title('å®‰å…¨è¾¹ç•Œå¯è§†åŒ–ï¼ˆä½™é‡è¶Šå¤§è¶Šå®‰å…¨ï¼‰', fontsize=12, fontweight='bold')
        ax5.set_xticks(x_pos)
        ax5.set_xticklabels(categories)
        ax5.legend(fontsize=9)
        ax5.grid(True, alpha=0.3, axis='y')

        # æ·»åŠ é¢œè‰²è¯´æ˜
        ax5.text(0.02, 0.98, 'ğŸ”´ å±é™©åŒºï¼ˆä½™é‡<1ï¼‰', transform=ax5.transAxes,
                fontsize=9, color='#C73E1D', va='top')
        ax5.text(0.02, 0.93, 'ğŸŸ  è­¦å‘ŠåŒºï¼ˆä½™é‡1-2ï¼‰', transform=ax5.transAxes,
                fontsize=9, color='#F18F01', va='top')
        ax5.text(0.02, 0.88, 'ğŸ”µ å®‰å…¨åŒºï¼ˆä½™é‡>2ï¼‰', transform=ax5.transAxes,
                fontsize=9, color='#2E86AB', va='top')

        # ===== å›¾6: è¯¦ç»†æŠ•ç¥¨æ•£ç‚¹å›¾ï¼ˆæ‰€æœ‰æŠ•ç¥¨ç‚¹ï¼‰ =====
        ax6 = fig.add_subplot(gs[1, 2])

        # æ£€æŸ¥æ˜¯å¦æœ‰æŠ•ç¥¨è¯¦æƒ…æ•°æ®
        has_vote_data = any(r.get('vote_details') for r in honest_results)

        if has_vote_data:
            # æ”¶é›†æ‰€æœ‰æŠ•ç¥¨è¯¦æƒ…
            for r in honest_results:
                malicious_ratio = r['malicious_ratio']
                quorum = r['quorum_threshold']
                vote_details = r.get('vote_details', [])

                for vd in vote_details:
                    y_count = vd['y_count']
                    is_success = vd['task_success']

                    # æ ¹æ®æ˜¯å¦æˆåŠŸè®¾ç½®é¢œè‰²
                    color = '#2E86AB' if is_success else '#C73E1D'
                    marker = 'o' if is_success else 'x'
                    alpha = 0.8 if is_success else 0.5

                    ax6.scatter(malicious_ratio, y_count,
                              marker=marker, s=100, color=color, alpha=alpha,
                              edgecolors='black', linewidth=0.5)

            # æ·»åŠ é˜ˆå€¼çº¿
            for i, r in enumerate(honest_results):
                quorum = r['quorum_threshold']
                ax6.axhline(y=quorum, xmin=(i-0.5)/len(malicious_ratios),
                           xmax=(i+0.5)/len(malicious_ratios),
                           color='r', linestyle='--', alpha=0.5, linewidth=2)

            ax6.set_xlabel('æ¶æ„èŠ‚ç‚¹æ¯”ä¾‹', fontsize=11)
            ax6.set_ylabel('å®é™…Yç¥¨æ•°', fontsize=11)
            ax6.set_title('æ¯æ¬¡æŠ•ç¥¨è¯¦ç»†åˆ†å¸ƒï¼ˆåœ†=æˆåŠŸï¼Œå‰=å¤±è´¥ï¼‰', fontsize=12, fontweight='bold')
            ax6.set_xticks(malicious_ratios)
            ax6.set_xticklabels([f'{int(r*100)}%' for r in malicious_ratios])

            # æ·»åŠ å›¾ä¾‹
            from matplotlib.lines import Line2D
            legend_elements = [
                Line2D([0], [0], marker='o', color='w', markerfacecolor='#2E86AB',
                       markersize=10, label='æˆåŠŸè¾¾æˆå…±è¯†', markeredgecolor='black'),
                Line2D([0], [0], marker='x', color='w', markerfacecolor='#C73E1D',
                       markersize=10, label='æœªè¾¾æˆå…±è¯†', markeredgecolor='black'),
                Line2D([0], [0], color='r', linestyle='--', label='2f+1é˜ˆå€¼')
            ]
            ax6.legend(handles=legend_elements, fontsize=9)
        else:
            # å¦‚æœæ²¡æœ‰æŠ•ç¥¨è¯¦æƒ…æ•°æ®ï¼Œæ˜¾ç¤ºæç¤ºä¿¡æ¯
            ax6.text(0.5, 0.5, 'æ— æŠ•ç¥¨è¯¦æƒ…æ•°æ®\nè¯·æ£€æŸ¥ä»»åŠ¡æ‰§è¡Œç»“æœ',
                    ha='center', va='center', fontsize=12,
                    transform=ax6.transAxes)
            ax6.set_title('æ¯æ¬¡æŠ•ç¥¨è¯¦ç»†åˆ†å¸ƒï¼ˆæ•°æ®ä¸å¯ç”¨ï¼‰', fontsize=12, fontweight='bold')
            ax6.set_xticks([])
            ax6.set_yticks([])

        ax6.grid(True, alpha=0.3)

        # ä¿å­˜å›¾è¡¨
        output_file = f"{plotter.output_dir}/{filename}"
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"  å·²ä¿å­˜: {output_file}")

        # ä¹Ÿä¿å­˜ç®€åŒ–ç‰ˆæœ¬
        simple_filename = f"fault_tolerance_latest_{llm_backend}.png"
        simple_output_file = f"{plotter.output_dir}/{simple_filename}"
        plt.savefig(simple_output_file, dpi=300, bbox_inches='tight')
        print(f"  å·²ä¿å­˜ï¼ˆç®€åŒ–åç§°ï¼‰: {simple_output_file}")

        plt.close()
